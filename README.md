# NLP
# ğŸ§  Natural Language Processing (NLP) Roadmap


---

<img src="Images/roadmap.png" alt="NLP Roadmap" width="600"/>

---

## ğŸ“Œ Part 1: Text Preprocessing using NLTK

### ğŸ”¹ Techniques Covered
- Tokenization
- Stemming (Porter, Regexp, Snowball)
- Lemmatization (WordNet)
- Stop Words Removal

---

### 1. ğŸ”¸ Tokenization
> Breaking down sentences into individual tokens (words or phrases).

<img src="Images/tokenization.png" alt="Tokenization" width="500"/>

---

### 2. ğŸ”¸ Stemming
> Reducing words to their root form.

| Stemmer Type       | Description                                         |
|--------------------|-----------------------------------------------------|
| **Porter Stemmer** | Automatic, fast, but lower accuracy                |
| **Regexp Stemmer** | Manual suffix removal, not automatic               |
| **Snowball Stemmer** | Improved accuracy over Porter                    |

---

### 3. ğŸ”¸ Lemmatization
> Converts words to their base dictionary form (lemma).

- Uses WordNetLemmatizer
- More accurate than stemming but slightly slower

---

### 4. ğŸ”¸ Stop Words Removal
> Removes common, insignificant words (like â€œisâ€, â€œtheâ€, â€œandâ€) to focus on meaningful text.

---

## ğŸ“Œ Part 2: Vectorization Techniques

### ğŸ”¹ Techniques Covered
- One Hot Encoding
- Bag of Words
- TF-IDF (Term Frequency - Inverse Document Frequency)

---

### 1. ğŸ”¸ One Hot Encoding
> Represents words as binary vectors based on vocabulary index.

<img src="Images/ohe.png" alt="One Hot Encoding" width="400"/>
<img src="Images/unique.png" alt="Vocabulary" width="300"/>

**Limitations:**
- Prone to **OOV (Out Of Vocabulary)** errors
- Does **not capture semantic meaning**

---

### 2. ğŸ”¸ Bag of Words
> Represents text by the frequency of words in a document.

*ğŸ–¼ï¸ Add Bag of Words diagram here*

---

### 3. ğŸ”¸ TF-IDF (Term Frequency-Inverse Document Frequency)
> Weights the word frequency by how unique a word is across documents.

*ğŸ–¼ï¸ Add TF-IDF illustration here*

---

## ğŸš€ Part 3: Deep Learning for NLP *(Coming Soon)*

- ğŸ”„ **RNN (Recurrent Neural Networks)**
- ğŸ” **LSTM (Long Short-Term Memory)**
- ğŸ”ƒ **Bidirectional LSTM**
- ğŸ”¤ **Word Embeddings (Word2Vec, GloVe)**
- ğŸ§  **Transformer models (optional)**

---

### ğŸ”„ 1. Recurrent Neural Networks (RNN)

> RNNs are designed for sequential data. They maintain a hidden state that captures information from previous time steps, making them suitable for tasks like sentiment analysis and text generation.

<img src="Images/rnn.png" alt="RNN Architecture" width="500"/>

**Limitations:**
- Struggles with long sequences due to vanishing gradients
- Hard to capture long-term dependencies

---

### ğŸ” 2. Long Short-Term Memory Networks (LSTM)

> LSTM is a type of RNN that solves the vanishing gradient problem using memory cells and gates (input, forget, and output gates).

<img src="Images/lstm.png" alt="LSTM Cell Architecture" width="500"/>

**Advantages:**
- Retains long-term dependencies
- Widely used in text classification, machine translation, and time-series prediction

---

### ğŸ”ƒ 3. Bidirectional LSTM

> Bidirectional LSTMs process the sequence in both forward and backward directions, providing context from past and future at each time step.

<img src="Images/bilstm.png" alt="Bidirectional LSTM" width="500"/>

**Use Cases:**
- Named Entity Recognition (NER)
- Question Answering
- Text Summarization

---

### ğŸ”¤ 4. Word Embeddings â€“ Word2Vec

> Word2Vec is a technique to represent words as dense vectors that capture their semantic meaning. Words with similar context have similar vectors.

#### Two architectures:
- **CBOW (Continuous Bag of Words):** Predicts the current word from surrounding context.
- **Skip-Gram:** Predicts surrounding context given the current word.

<img src="Images/word2vec.png" alt="Word2Vec" width="500"/>

**Advantages:**
- Captures semantic similarity  
  (e.g., `"king" - "man" + "woman" â‰ˆ "queen"`)
- Reduces dimensionality and sparsity compared to one-hot or TF-IDF

---

## âœ… Summary

| Technique          | Handles Sequence | Retains Context         | Semantic Understanding |
|--------------------|------------------|--------------------------|-------------------------|
| RNN                | âœ…               | âœ… (short-term)          | âŒ                      |
| LSTM               | âœ…               | âœ… (long-term)           | âŒ                      |
| BiLSTM             | âœ…               | âœ… (bi-directional)      | âŒ                      |
| Word2Vec           | âŒ               | âŒ                       | âœ…                      |
